# np_nets
Neural network experiments written purely in numpy

1. Learning backprop with the MNIST classification task
  * [Jupyter notebook](https://nbviewer.jupyter.org/github/greydanus/np_nets/blob/master/mnist_nn.ipynb)
2. Better version of learning backprop...lacks markdown explanations...doesn't have a bug though
  * [Jupyter notebook](https://nbviewer.jupyter.org/github/greydanus/np_nets/blob/master/numpy-mnist.ipynb)
3. Synthetic gradients with the MNIST classification task
  * [Jupyter notebook](https://nbviewer.jupyter.org/github/greydanus/np_nets/blob/master/synthetic-gradients.ipynb)
  * also check out the minimalist [145-line Gist](https://gist.github.com/greydanus/1cb90875f24015660ae91fa637f167a9) for this project
  * inspired by [this Google DeepMind paper](https://deepmind.com/blog/decoupled-neural-networks-using-synthetic-gradients/)
4. Hebbian learning (for a Dartmouth class)
  * [Jupyter notebook](https://nbviewer.jupyter.org/github/greydanus/np_nets/blob/master/hebb-pset5.ipynb)
  * For Human Memory (PSYC 051.09) taught by [Jeremy Manning](http://www.context-lab.com/)
5. "U loss" learning
  * I test an ansatz for layer-wise training of neural networks. **It didn't work**. That's how research goes.
  * Folder is [here](https://github.com/greydanus/np_nets/blob/master/uloss)
