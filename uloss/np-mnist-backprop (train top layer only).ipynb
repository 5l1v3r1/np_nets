{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple NN for MNIST (NumPy) \\*\\*Only train top layer\\*\\*\n",
    "Sam Greydanus. 27 April 2017. MIT License.\n",
    "\n",
    "### Comments\n",
    "When I explore alternative methods to training neural nets with backprop (e.g. the U loss method), I will use this notebook as a baseline for NN performance on the MNIST when only the top layer is trained with backprop. Max accuracy usually ~60-70% if you train it 20000+ steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "%matplotlib inline\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets('MNIST_data', one_hot=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lr = 1e-3\n",
    "batch_size = 32\n",
    "global_step = 0\n",
    "print_every = 250\n",
    "total_steps = 10000\n",
    "\n",
    "D_hidden = 128 # hidden layer size\n",
    "D_side = 28\n",
    "D_img = D_side**2 # dimensionality of image\n",
    "D_labels = 10 # number of labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 : (784, 128)\n",
      "b1 : (1, 128)\n",
      "W2 : (128, 128)\n",
      "b2 : (1, 128)\n",
      "W3 : (128, 10)\n",
      "b3 : (1, 10)\n"
     ]
    }
   ],
   "source": [
    "model = {}\n",
    "# first layer\n",
    "model['W1'] = np.random.randn(D_img,D_hidden) / np.sqrt(D_hidden) # Xavier initialization\n",
    "model['b1'] = np.random.randn(1,D_hidden) / np.sqrt(D_hidden)\n",
    "#second layer\n",
    "model['W2'] = np.random.randn(D_hidden,D_hidden) / np.sqrt(D_hidden)\n",
    "model['b2'] = np.random.randn(1,D_hidden) / np.sqrt(D_hidden)\n",
    "#third layer\n",
    "model['W3'] = np.random.randn(D_hidden,D_labels) / np.sqrt(D_labels)\n",
    "model['b3'] = np.random.randn(1,D_labels) / np.sqrt(D_labels)\n",
    "\n",
    "for (k,v) in model.items(): print(\"{} : {}\".format(k, v.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def xW_plus_b(x, W, b):\n",
    "    return np.dot(x,W) + b # in some cases you can even drop the bias b\n",
    "\n",
    "def relu(x):\n",
    "    x[x<0] = 0\n",
    "    return x\n",
    "\n",
    "def softmax(x):\n",
    "    maxes = np.amax(x, axis=1, keepdims=True)\n",
    "    e = np.exp(x - maxes) # improves numerics\n",
    "    dist = e / np.sum(e, axis=1, keepdims=True)\n",
    "    return dist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backward functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dsoftmax(h, y, batch_size):\n",
    "    h[range(batch_size),y] -= 1\n",
    "    return h/y.shape[0] # divide by batch size\n",
    "\n",
    "def drelu(dz, h):\n",
    "    dz[h <= 0] = 0 # backprop relu\n",
    "    return dz\n",
    "\n",
    "def dxW_plus_b(dh, W):\n",
    "    return np.dot(dh, W.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def forward(X, model):\n",
    "    # evaluate class scores, [N x K]\n",
    "    hs = [] # we'll need the h's for computing gradients\n",
    "    \n",
    "    z1 = xW_plus_b(X, model['W1'], model['b1'])\n",
    "    h1 = relu(z1) ; hs.append(h1)\n",
    "    \n",
    "    z2 = xW_plus_b(h1, model['W2'], model['b2'])\n",
    "    h2 = relu(z2) ; hs.append(h2)\n",
    "    \n",
    "    z3 = xW_plus_b(h2, model['W3'], model['b3'])\n",
    "    h3 = z3 ; hs.append(h3)\n",
    "    \n",
    "    probs = softmax(h3)\n",
    "    return probs, hs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# evaluate test set accuracy\n",
    "def test_accuracy(model):\n",
    "    X = mnist.test.images\n",
    "    y = mnist.test.labels\n",
    "    scores, _ = forward(X, model)\n",
    "    predicted_class = np.argmax(scores, axis=1)\n",
    "    return (np.mean(predicted_class == y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def backward(y, probs, X, hs, model):\n",
    "    grads = { k : np.zeros_like(v) for k,v in model.items() }\n",
    "    dh3 = dsoftmax(probs, y, batch_size)\n",
    "    \n",
    "    # third hidden layer\n",
    "    grads['W3'] = np.dot(hs[-2].T, dh3)\n",
    "    grads['b3'] = np.sum(dh3, axis=0, keepdims=True)\n",
    "    \n",
    "    # second hidden layer\n",
    "    dh2 = dxW_plus_b(dh3, model['W3'])\n",
    "    dh2 = drelu(dh2, hs[-2]) # backprop through relu\n",
    "    grads['W2'] = np.zeros_like(np.dot(hs[-3].T, dh2))\n",
    "    grads['b2'] = np.zeros_like(np.sum(dh2, axis=0, keepdims=True))\n",
    "\n",
    "    # first hidden layer\n",
    "    dh1 = dxW_plus_b(dh2, model['W2'])\n",
    "    dh1 = drelu(dh1, hs[-3]) # backprop through relu\n",
    "    grads['W1'] = np.zeros_like(np.dot(X.T, dh1))\n",
    "    grads['b1'] = np.zeros_like(np.sum(dh1, axis=0, keepdims=True))\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 10.38%\n",
      "\tstep 0: loss: 3.4576\n",
      "\tstep 250: loss: 2.8574\n",
      "\tstep 500: loss: 2.6067\n",
      "\tstep 750: loss: 2.5110\n",
      "accuracy: 13.13%\n",
      "\tstep 1000: loss: 2.4569\n",
      "\tstep 1250: loss: 2.4019\n",
      "\tstep 1500: loss: 2.3503\n",
      "\tstep 1750: loss: 2.3251\n",
      "accuracy: 15.67%\n",
      "\tstep 2000: loss: 2.2984\n",
      "\tstep 2250: loss: 2.2698\n",
      "\tstep 2500: loss: 2.2469\n",
      "\tstep 2750: loss: 2.2201\n",
      "accuracy: 18.51%\n",
      "\tstep 3000: loss: 2.1846\n",
      "\tstep 3250: loss: 2.1728\n",
      "\tstep 3500: loss: 2.1402\n",
      "\tstep 3750: loss: 2.1044\n",
      "accuracy: 22.74%\n",
      "\tstep 4000: loss: 2.0981\n",
      "\tstep 4250: loss: 2.0762\n",
      "\tstep 4500: loss: 2.0554\n",
      "\tstep 4750: loss: 2.0238\n",
      "accuracy: 27.45%\n",
      "\tstep 5000: loss: 1.9995\n",
      "\tstep 5250: loss: 1.9687\n",
      "\tstep 5500: loss: 1.9518\n",
      "\tstep 5750: loss: 1.9590\n",
      "accuracy: 32.61%\n",
      "\tstep 6000: loss: 1.9202\n",
      "\tstep 6250: loss: 1.9119\n",
      "\tstep 6500: loss: 1.8824\n",
      "\tstep 6750: loss: 1.8732\n",
      "accuracy: 37.22%\n",
      "\tstep 7000: loss: 1.8654\n",
      "\tstep 7250: loss: 1.8353\n",
      "\tstep 7500: loss: 1.8161\n",
      "\tstep 7750: loss: 1.7960\n",
      "accuracy: 41.05%\n",
      "\tstep 8000: loss: 1.7919\n",
      "\tstep 8250: loss: 1.7736\n",
      "\tstep 8500: loss: 1.7631\n",
      "\tstep 8750: loss: 1.7421\n",
      "accuracy: 44.15%\n",
      "\tstep 9000: loss: 1.7259\n",
      "\tstep 9250: loss: 1.7065\n",
      "\tstep 9500: loss: 1.7097\n",
      "\tstep 9750: loss: 1.6928\n",
      "accuracy: 46.97%\n",
      "\tstep 10000: loss: 1.6824\n"
     ]
    }
   ],
   "source": [
    "# generic train loop\n",
    "running_loss = None ; interp = 0.99\n",
    "for global_step in range(global_step, total_steps+global_step+1):\n",
    "    \n",
    "    # forward\n",
    "    X, y = mnist.train.next_batch(batch_size)\n",
    "    probs, hs = forward(X, model)\n",
    "    \n",
    "    y_logprobs = -np.log(probs[range(batch_size),y]) # cross-entropy loss\n",
    "    loss = np.sum(y_logprobs)/batch_size\n",
    "    running_loss = loss if running_loss is None else interp*running_loss + (1-interp)*loss\n",
    "    \n",
    "    # backward\n",
    "    grads = backward(y, probs, X, hs, model) # data gradients\n",
    "    model = {k : model[k] - lr*grads[k] for (k,v) in grads.items()} # update parameters\n",
    "\n",
    "    # ======== DISPLAY PROGRESS ======== #\n",
    "    if global_step % print_every == 0:\n",
    "        if global_step / 4 % print_every == 0:\n",
    "            print('accuracy: {:.2f}%'.format(100*test_accuracy(model)))\n",
    "        print('\\tstep {}: loss: {:.4f}'\n",
    "              .format(global_step, running_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
