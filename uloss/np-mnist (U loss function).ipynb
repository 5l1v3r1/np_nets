{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple NN for MNIST: Trained with 'U' Loss function\n",
    "Sam Greydanus. 27 April 2017. MIT License.\n",
    "\n",
    "The idea behind the 'U loss function' is to penalize neurons when their activations cause the next layer of neurons to fire moderately but reward them when their activations cause the next layer of neurons to fire very strongly. No reward is given for low activations.\n",
    "\n",
    "The toughest part is defining what \"strong activations\" vs \"moderate activations\" vs \"low activations\" means mathematically. Here I keep a running approximation of the mean and variance of each neurons' activations. Then I compare their current activation ($x$) to their historical mean ($\\mu$) and historical variance ($\\sigma$) using the `uloss(x,mus,sigmas)` function (in the 'Forward functions' section).\n",
    "\n",
    "Once I have the equivalent of a cost function for each activation, I backprop these values through the current layer and thus them to update the previous layer's weights. In this way, the previous layer is rewarded for producing strong/low activations in the next layer and penalized for producing moderate activations.\n",
    "\n",
    "Ideas adapted from personal correspondance with Kenneth Norman...though I'm really not sure if this is what he had in mind\n",
    "\n",
    "### Findings\n",
    "When you compare w/ the backprop version of this notebook and the version where I only do backprop on the top layer, this method doesn't really help with training. Explanations: 1) I messed something up in my code 2) I made some poor assumptions when translating the idea into an code implementation 3) this is not a good way of training neural nets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "%matplotlib inline\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets('MNIST_data', one_hot=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lr = 1e-3\n",
    "batch_size = 32\n",
    "global_step = 0\n",
    "print_every = 500\n",
    "total_steps = 10000\n",
    "\n",
    "D_hidden = 128 # hidden layer size\n",
    "D_side = 28\n",
    "D_img = D_side**2 # dimensionality of image\n",
    "D_labels = 10 # number of labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 : (784, 128)\n",
      "b1 : (1, 128)\n",
      "W2 : (128, 128)\n",
      "b2 : (1, 128)\n",
      "W3 : (128, 10)\n",
      "b3 : (1, 10)\n"
     ]
    }
   ],
   "source": [
    "model = {}\n",
    "# first layer\n",
    "model['W1'] = np.random.randn(D_img,D_hidden) / np.sqrt(D_hidden) # Xavier initialization\n",
    "model['b1'] = np.random.randn(1,D_hidden) / np.sqrt(D_hidden)\n",
    "#second layer\n",
    "model['W2'] = np.random.randn(D_hidden,D_hidden) / np.sqrt(D_hidden)\n",
    "model['b2'] = np.random.randn(1,D_hidden) / np.sqrt(D_hidden)\n",
    "#third layer\n",
    "model['W3'] = np.random.randn(D_hidden,D_labels) / np.sqrt(D_labels)\n",
    "model['b3'] = np.random.randn(1,D_labels) / np.sqrt(D_labels)\n",
    "\n",
    "for (k,v) in model.items(): print(\"{} : {}\".format(k, v.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def xW_plus_b(x, W, b):\n",
    "    return np.dot(x,W) + b # in some cases you can even drop the bias b\n",
    "\n",
    "def relu(x):\n",
    "    x[x<0] = 0\n",
    "    return x\n",
    "\n",
    "def softmax(x):\n",
    "    maxes = np.amax(x, axis=1, keepdims=True)\n",
    "    e = np.exp(x - maxes) # improves numerics\n",
    "    dist = e / np.sum(e, axis=1, keepdims=True)\n",
    "    return dist\n",
    "\n",
    "def gaussian(x, mu, sigma):\n",
    "    a = 1./(sigma*2.5+1e-6)\n",
    "    return a * np.exp(-.5*((x-mu)/(sigma+1e-6))**2)\n",
    "\n",
    "def uloss(x, mus, sigmas):\n",
    "    clip = (x<(m+3*s)) * (x>(m-s))\n",
    "    d = clip * x + (1-clip)*np.ones_like(x)*(m-s)\n",
    "    return (-gaussian(d,m,s)+.25)*.001 # note: these are some arbitrarily chosen constants that SEEMED like good choices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZkAAAEICAYAAACNn4koAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nN55knHPOxY0nGeecc3Hz/wFFFWKJbbex/wAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11198e278>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "k = 100\n",
    "x = np.linspace(-4,4,k)\n",
    "m = 0 ; s = 1# np.ones(k)*.01 ; np.linspace(-1,1,k)\n",
    "plt.title(\"The U loss function\")\n",
    "plt.plot(x,uloss(x,m,s), 'k-') ; plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backward functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dsoftmax(h, y, batch_size):\n",
    "    h[range(batch_size),y] -= 1\n",
    "    return h/y.shape[0] # divide by batch size\n",
    "\n",
    "def drelu(dz, h):\n",
    "    dz[h <= 0] = 0 # backprop relu\n",
    "    return dz\n",
    "\n",
    "def dxW_plus_b(dh, W):\n",
    "    return np.dot(dh, W.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def forward(X, model):\n",
    "    # evaluate class scores, [N x K]\n",
    "    hs = [] # we'll need the h's for computing gradients\n",
    "    \n",
    "    z1 = xW_plus_b(X, model['W1'], model['b1'])\n",
    "    h1 = relu(z1) ; hs.append(h1)\n",
    "    \n",
    "    z2 = xW_plus_b(h1, model['W2'], model['b2'])\n",
    "    h2 = relu(z2) ; hs.append(h2)\n",
    "    \n",
    "    z3 = xW_plus_b(h2, model['W3'], model['b3'])\n",
    "    h3 = z3 ; hs.append(h3)\n",
    "    \n",
    "    probs = softmax(h3)\n",
    "    \n",
    "    return probs, hs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# evaluate test set accuracy\n",
    "def test_accuracy(model):\n",
    "    X = mnist.test.images\n",
    "    y = mnist.test.labels\n",
    "    scores, _ = forward(X, model)\n",
    "    predicted_class = np.argmax(scores, axis=1)\n",
    "    return (np.mean(predicted_class == y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def backward(y, probs, stats, X, hs, model):\n",
    "    grads = { k : np.zeros_like(v) for k,v in model.items() }\n",
    "    \n",
    "    # third hidden layer\n",
    "    dh3 = dsoftmax(probs, y, batch_size)\n",
    "    \n",
    "    grads['W3'] = np.dot(hs[-2].T, dh3)\n",
    "    grads['b3'] = np.sum(dh3, axis=0, keepdims=True)\n",
    "    \n",
    "    # second hidden layer\n",
    "    dh3 = uloss(hs[-1],stats['mus'][-1], stats['sigmas'][-1])\n",
    "    \n",
    "    dh2 = dxW_plus_b(dh3, model['W3']) #; print(dh2)\n",
    "    dh2 = drelu(dh2, hs[-2]) # backprop through relu\n",
    "    grads['W2'] = np.dot(hs[-3].T, dh2)\n",
    "    grads['b2'] = np.sum(dh2, axis=0, keepdims=True)\n",
    "\n",
    "    # first hidden layer\n",
    "    dh2 = uloss(hs[-2],stats['mus'][-2], stats['sigmas'][-2])\n",
    "    \n",
    "    dh1 = dxW_plus_b(dh2, model['W2'])\n",
    "    dh1 = drelu(dh1, hs[-3]) # backprop through relu\n",
    "    grads['W1'] = np.dot(X.T, dh1)\n",
    "    grads['b1'] = np.sum(dh1, axis=0, keepdims=True)\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 10.56%\n",
      "\tstep 0: loss: 3.6430\n",
      "\tstep 500: loss: 2.9194\n",
      "\tstep 1000: loss: 2.6196\n",
      "\tstep 1500: loss: 2.4482\n",
      "accuracy: 15.70%\n",
      "\tstep 2000: loss: 2.3614\n",
      "\tstep 2500: loss: 2.3150\n",
      "\tstep 3000: loss: 2.2570\n",
      "\tstep 3500: loss: 2.2015\n",
      "accuracy: 21.51%\n",
      "\tstep 4000: loss: 2.1697\n",
      "\tstep 4500: loss: 2.1349\n",
      "\tstep 5000: loss: 2.0903\n",
      "\tstep 5500: loss: 2.0612\n",
      "accuracy: 29.00%\n",
      "\tstep 6000: loss: 2.0206\n",
      "\tstep 6500: loss: 2.0034\n",
      "\tstep 7000: loss: 1.9812\n",
      "\tstep 7500: loss: 1.9357\n",
      "accuracy: 34.78%\n",
      "\tstep 8000: loss: 1.9341\n",
      "\tstep 8500: loss: 1.9106\n",
      "\tstep 9000: loss: 1.8904\n",
      "\tstep 9500: loss: 1.8721\n",
      "accuracy: 39.48%\n",
      "\tstep 10000: loss: 1.8691\n"
     ]
    }
   ],
   "source": [
    "# generic train loop\n",
    "interp = 0.99\n",
    "stats = {'mus':None, 'sigmas':None}\n",
    "running_loss = None\n",
    "\n",
    "for global_step in range(global_step, total_steps+global_step+1):\n",
    "    \n",
    "    # forward\n",
    "    X, y = mnist.train.next_batch(batch_size)\n",
    "    probs, hs = forward(X, model)\n",
    "    \n",
    "    y_logprobs = -np.log(probs[range(batch_size),y]) # cross-entropy loss\n",
    "    loss = np.sum(y_logprobs)/batch_size\n",
    "    \n",
    "    # feedforward stats (keep running averages of each neuron's statistics over time)\n",
    "    mus = [np.asarray(h) for h in hs]\n",
    "    sigmas = [np.ones_like(v)*.001 for v in mus] if stats['mus'] is None else [np.abs(stats['mus'][i] - mus[i]) for i in range(len(mus))]\n",
    "    stats['mus'] = mus if stats['mus'] is None else [interp*stats['mus'][i] + (1.-interp)*mus[i] for i in range(len(mus))]\n",
    "    stats['sigmas'] = sigmas if stats['sigmas'] is None else [interp*stats['sigmas'][i] + (1.-interp)*sigmas[i] for i in range(len(sigmas))]\n",
    "    \n",
    "    running_loss = loss if running_loss is None else interp*running_loss + (1.-interp)*loss\n",
    "    \n",
    "    # backward\n",
    "    grads = backward(y, probs, stats, X, hs, model) # data gradients\n",
    "    model = {k : model[k] - lr*grads[k] for (k,v) in grads.items()} # update parameters\n",
    "\n",
    "    # ======== DISPLAY PROGRESS ======== #\n",
    "    if global_step % print_every == 0:\n",
    "        if global_step / 4 % print_every == 0:\n",
    "            print('accuracy: {:.2f}%'.format(100*test_accuracy(model)))\n",
    "        print('\\tstep {}: loss: {:.4f}'\n",
    "              .format(global_step, running_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
